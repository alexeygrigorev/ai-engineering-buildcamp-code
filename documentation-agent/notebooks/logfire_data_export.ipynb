{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca97c34-f9ab-4452-aeae-b3f8894f52bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55dab105-ff8d-4950-970c-f613269bb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from logfire.query_client import LogfireQueryClient\n",
    "\n",
    "read_token = os.getenv('LOGFIRE_READ_TOKEN')\n",
    "logfire_query_client = LogfireQueryClient(read_token=read_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84104026-94de-492e-9512-67cc652fb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_rows = logfire_query_client.query_json_rows(\n",
    "    sql=\"\"\"\n",
    "    SELECT\n",
    "        trace_id,\n",
    "        start_timestamp,\n",
    "        duration\n",
    "    FROM records\n",
    "    WHERE span_name = 'streamlit_session'\n",
    "    ORDER BY start_timestamp DESC\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "737c3ae2-4d4c-4ec5-b03a-d3b59aec6dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['019c8f3f6447f880e1a12852e646af68',\n",
       " '019c8f3d662b1302eeb88ce03a1504a2',\n",
       " '019c8f3a23e4cc6bd259a5a00f708b29',\n",
       " '019c8f2d3ef001f77db918166936d315',\n",
       " '019c8f2c899df265457bc2814ffe975b']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_ids = [r['trace_id'] for r in trace_rows['rows']]\n",
    "trace_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "818e7618-480f-41fc-8241-50bb0d968103",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id = '019c8f3a23e4cc6bd259a5a00f708b29'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e2dc481-39cf-46b6-be82-0324b9df71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_row = logfire_query_client.query_json_rows(\n",
    "    sql=f\"\"\"\n",
    "    SELECT\n",
    "        attributes->'pydantic_ai.all_messages' as all_messages,\n",
    "        attributes->>'gen_ai.usage.input_tokens' as input_tokens,\n",
    "        attributes->>'gen_ai.usage.output_tokens' as output_tokens\n",
    "    FROM records\n",
    "    WHERE trace_id = '{trace_id}'\n",
    "      AND span_name = 'agent run'\n",
    "    ORDER BY start_timestamp DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbad37db-90ac-43bf-82dd-767de0d39bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages = run_row['rows'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "451eb5cc-491a-4f5b-a2f0-a1297cd1031b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'parts': [{'type': 'text',\n",
       "   'content': 'How do I customize my LLM judge prompts?\\n\\n'}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_messages['all_messages'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27686ce8-417d-4492-9131-b3bd08478537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dac50a28-79d0-427c-b491-49c22fe0dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trace_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "155fcd50-6a0a-48a7-8e0c-dd07d216f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = trace_replay.fetch_trace(trace_id, logfire_query_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02ed8f72-b506-467b-9610-87eca1eb3997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'parts': [{'type': 'text',\n",
       "   'content': 'How do I customize my LLM judge prompts?\\n\\n'}]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace.all_messages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab0af9e7-431f-462d-8cf0-a51243ece35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import RAGResponse\n",
    "run = trace_replay.trace_to_run_result(trace, output_type=RAGResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94ca5ced-3699-4d8e-8db1-6ca880960616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Common errors when using an LLM as a judge include:\\n\\n1. **Inaccurate Responses**: The LLM may produce incorrect evaluations due to insufficient context or ambiguous prompts. It's essential to provide clear and specific criteria in the evaluation prompts.\\n\\n2. **Overfitting to Training Data**: If the LLM has been trained on biased or unrepresentative datasets, it may favor evaluations based on that training, leading to skewed results.\\n\\n3. **Difficulty with Open-ended Prompts**: LLMs may struggle with open-ended evaluations where specific reference answers are not available. In such scenarios, establishing clear, custom criteria is vital to guide the evaluations.\\n\\n4. **Lack of Detailed Reasoning**: Sometimes, LLMs may fail to provide sufficient reasoning behind their judgments, making it difficult to understand why a particular evaluation was made. Always encourage detailed explanations in the prompts.\\n\\n5. **Variability in Output**: Due to the non-deterministic nature of LLMs, the same input may produce different outputs across runs. This variability can cause inconsistencies in evaluations unless controlled properly.\\n\\n6. **Inadequate Prompt Design**: Poorly structured prompts can lead to misunderstandings. Ensure prompts are well-designed, concise, and include enough context to aid the LLM in making accurate assessments.\\n\\n7. **Failing to Iterate and Refine Prompts**: Neglecting to continually refine and improve the evaluation prompts based on feedback and results can limit the effectiveness of the LLM judge.\\n\\nTo mitigate these issues, it’s essential to continuously evaluate the LLM’s performance, adjust your prompts as necessary, and validate the evaluations against a reliable reference or expert judgment where possible.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.output.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82d976a6-f626-4053-9a43-5cf5778b6197",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = trace_replay.fetch_traces(trace_ids, logfire_query_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0fea2b33-957a-443e-8186-c7aa1554a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "\n",
    "\n",
    "for trace in traces.values():\n",
    "    run = trace_replay.trace_to_run_result(trace)\n",
    "    runs.append(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb2beb19-9601-45bd-bacc-ee3eee01af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e80a2e-dd9c-431d-a746-ad0882a3ebcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d774617d-9482-400d-83f3-3b62cb99d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/logs.bin', 'wb') as f_out:\n",
    "    pickle.dump(runs, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2a2eb90-50fc-4171-b282-cca46efac2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/logs.bin', 'rb') as f_in:\n",
    "    runs = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b7da7-881f-467d-a6f9-f52af82e4a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
