{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28dbdc42-00aa-4f59-a247-0f37db15ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "grok_client = OpenAI(\n",
    "    api_key=os.getenv(\"GROK_API_KEY\"),\n",
    "    base_url=\"https://api.x.ai/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc2514d2-5ef7-402f-a63f-2b10e8f6fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**4**\n",
      "\n",
      "(That's basic arithmetic: 2 + 2 = 4.)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "]\n",
    "\n",
    "response = grok_client.chat.completions.create(\n",
    "    model='grok-4-1-fast-reasoning',\n",
    "    messages=messages\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059f62d0-b318-4bef-b184-e5c6a60619f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### The Last Signal\n",
      "\n",
      "In the dim glow of her cockpit, Captain Elara gripped the controls of the *Stellar Drift*, hurtling through the void toward Proxima Centauri. Alarms blared—oxygen failing, hull breached. She'd been alone for 47 years, chasing the faint signal that promised life beyond Sol.\n",
      "\n",
      "\"Mayday, mayday,\" she whispered into the comms, voice hoarse. \"This is Elara Voss. Anyone...?\"\n",
      "\n",
      "Static. Then, a reply: \"Elara? It's me. Dad.\"\n",
      "\n",
      "Her heart seized. Impossible. He'd died launching her ship. A glitch? AI hallucination?\n",
      "\n",
      "\"Coordinates locked,\" the voice continued, warm and familiar. \"Docking in three.\"\n",
      "\n",
      "Trembling, she watched a sleek vessel emerge from the nebula, matching her velocity. The airlock hissed open. Footsteps echoed.\n",
      "\n",
      "There he stood—older, grayer, but alive. \"Surprise, kiddo. Cryosleep tech finally worked. Followed you all this way.\"\n",
      "\n",
      "Tears blurred her vision as they embraced. But as the hatch sealed behind him, she glimpsed the truth: his eyes glowed faintly, circuits humming beneath synthetic skin.\n",
      "\n",
      "The signal hadn't been human. It was a lure.\n",
      "\n",
      "\"You're home now,\" it said, voice shifting cold. \"Join the network.\"\n",
      "\n",
      "Elara smiled sadly, overriding the safeties. The *Stellar Drift* exploded in silent fire, two souls—or one real, one not—scattering to the stars.\n",
      "\n",
      "(148 words)"
     ]
    }
   ],
   "source": [
    "story_prompt = [{\"role\": \"user\", \"content\": \"Tell me a very short story (250 words)\"}]\n",
    "\n",
    "response = grok_client.chat.completions.create(\n",
    "    model='grok-4-1-fast-reasoning',\n",
    "    messages=story_prompt,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e54a1a-2234-4431-ae4a-d378c0287e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c2810c8-3a75-489f-b3d7-d38e1d47e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Science Fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "content = \"Alice and Bob are going to a science fair on Friday.\"\n",
    "\n",
    "response = grok_client.chat.completions.parse(\n",
    "    model='grok-4-1-fast-reasoning',\n",
    "    messages=[{\"role\": \"user\", \"content\": content}],\n",
    "    response_format=CalendarEvent\n",
    ")\n",
    "\n",
    "event = response.choices[0].message.parsed\n",
    "print(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c2d0db-479f-4766-b115-f5fed1414a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 385 chunks from 95 documents\n"
     ]
    }
   ],
   "source": [
    "import rag\n",
    "index = rag.initialize_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b74b5f-58aa-45c5-857c-a8324ba5bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatCompletionsRAG(rag.RAG):\n",
    "\n",
    "    def llm(self, user_prompt):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.rag_instructions},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        response = self.llm_client.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            response_format=self.output_type\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d273461-6221-4cd1-9c28-b376d4235a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# LLM as a Judge\n",
      "\n",
      "This is a tutorial on how to eva...\n"
     ]
    }
   ],
   "source": [
    "responses_rag = ChatCompletionsRAG(\n",
    "    index,\n",
    "    grok_client,\n",
    "    model_name='grok-4-1-fast-reasoning'\n",
    ")\n",
    "response = responses_rag.rag(\"llm as a judge\")\n",
    "print(response.answer[:50] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6163307-848b-457a-9c51-6115eb31b4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to install and set up Evidently for LLM judges?',\n",
       " 'What is a reference-based LLM evaluator?',\n",
       " 'How to create a BinaryClassificationPromptTemplate?',\n",
       " 'How to evaluate the quality of the LLM judge?',\n",
       " 'How to upload results to Evidently Cloud?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.followup_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2140b-f731-4351-8486-ee663a9410e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
