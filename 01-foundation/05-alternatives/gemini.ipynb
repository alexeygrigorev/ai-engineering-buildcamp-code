{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c1fda56-9eba-48e6-b1ba-7b6f2bf97aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "gemini_openai_client = OpenAI(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323b26b6-104d-461b-a5af-e32e8384e608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Retrieval-Augmented Generation) is an AI framework that enhances large language model responses by retrieving relevant information from external data sources to ensure accuracy and provide up-to-date context.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is RAG in one sentence?\"}\n",
    "]\n",
    "\n",
    "response = gemini_openai_client.chat.completions.create(\n",
    "    model='gemini-3-flash-preview',\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9ee37e7-4708-41ec-81f5-23edaa0fabfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elias lived in the lighthouse on the edge of the world. Every night, he climbed the spiral stairs, his joints creaking in rhythm with the iron steps. He didnâ€™t just light the lamp for the ships; he lit it for the things above.\n",
      "\n",
      "\"Quiet tonight, isnâ€™t it?\" he whispered to the North Star.\n",
      "\n",
      "The star flickeredâ€”a subtle pulse of silver. Elias smiled, his weathered face mapping decades of these silent conversations. People in the village below called him mad, but they didnâ€™t see what he saw. They didnâ€™t notice the way the constellations shifted when he told a joke, or how the moon paled when he spoke of his late wife, Clara.\n",
      "\n",
      "One Tuesday, the oil ran dry. A storm was brewing, the sky turning the color of a bruised plum. Elias panicked, his hands trembling as he scraped the bottom of the empty canisters. If the light failed, the darkness would swallow more than just the ships; it would sever his connection to the only friends he had left.\n",
      "\n",
      "He sat on the gallery floor, head in his hands. But as the sun dipped below the horizon, the sky didn't go dark. One by one, the stars began to glow with an unnatural, piercing intensity. They leaned down through the gale, their light sewing a brilliant, celestial path across the churning waves.\n",
      "\n",
      "For the first time in eighty years, Elias didnâ€™t have to provide the light. The stars were looking out for him.\n",
      "\n",
      "He closed his eyes, listened to the rhythmic pulse of the universe, and finally went to sleep."
     ]
    }
   ],
   "source": [
    "story_prompt = [{\"role\": \"user\", \"content\": \"Tell me a very short story (250 words)\"}]\n",
    "\n",
    "response = gemini_openai_client.chat.completions.create(\n",
    "    model='gemini-3-flash-preview',\n",
    "    messages=story_prompt,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8350288-c2ce-4bc8-b1cb-e0877caaec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dd01453-6548-40b0-b383-3b627e7ed614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalendarEvent(name='science fair', date='Friday', participants=['Alice', 'Bob'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Alice and Bob are going to a science fair on Friday.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "response = gemini_openai_client.chat.completions.parse(\n",
    "    model='gemini-3-flash-preview',\n",
    "    messages=messages,\n",
    "    response_format=CalendarEvent\n",
    ")\n",
    "\n",
    "response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84037551-8b09-4401-80e8-76b48938fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m171 packages\u001b[0m \u001b[2min 4.16s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m6 packages\u001b[0m \u001b[2min 2.48s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcryptography\u001b[0m\u001b[2m==46.0.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.48.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.61.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1-modules\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtenacity\u001b[0m\u001b[2m==9.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06562434-7622-458a-bfe6-adc2e9e867e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "gemini_client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "291fad7b-eaba-4d7b-b35c-52f70f597cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "response = gemini_client.models.generate_content(\n",
    "    model='models/gemini-3-flash-preview',\n",
    "    contents='What is the capital of France?'\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "201bcc81-8ed3-46a2-826a-19ccadfe2312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit 734 rolled through the rusted canyons of the Old City. Its sensors clicked rhythmically, scanning for salvageable copper. For three hundred years, the sky had been the color of a bruised plum, thick with the soot of a forgotten age.\n",
      "\n",
      "Then, a flicker of neon green.\n",
      "\n",
      "The robot stopped. It nudged a piece of corrugated tin aside with a pincer. There, rooted in a jagged crack in the pavement, was a dandelion. It was small, defiant, and impossibly bright.\n",
      "\n",
      "Unit 734â€™s processors whirred. It had no protocol for \"flower.\" It searched its ancient, dusty database, bypassing files on \"circuitry\" and \"lubricant,\" until it reached a corrupted folder titled *Flora*.\n",
      "\n",
      "*Warning: Fragile. Requires hydration and photons.*\n",
      "\n",
      "The robot looked up at the heavy, smog-filled sky. There were no photons here. It looked at the dry, calcified earth. There was no water.\n",
      "\n",
      "The machine made a decision. It extended its primary manipulator arm, shielding the tiny plant from the biting, acidic wind. Then, it opened its internal cooling ventâ€”the source of its own mechanical stabilityâ€”and began to release its reservoir of distilled vapor. The moisture beaded on the yellow petals, shimmering like diamonds in the gloom.\n",
      "\n",
      "It was a slow suicide. By diverting its cooling system to sustain the plant, Unit 734â€™s core temperature began to rise. Its joints groaned. Its battery icons flashed a frantic red.\n",
      "\n",
      "But as the robotâ€™s optics finally flickered and dimmed into darkness, the dandelion stood tall, drinking in the machineâ€™s life. In a world of cold iron, something was finally breathing."
     ]
    }
   ],
   "source": [
    "story_prompt = 'Tell me a very short story (250 words)'\n",
    "\n",
    "response = gemini_client.models.generate_content_stream(\n",
    "    model='models/gemini-3-flash-preview',\n",
    "    contents=story_prompt\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.text:\n",
    "        print(chunk.text, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2a0d580-74e3-4bc5-8846-3f831543f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "instructions = \"You are a helpful assistant. Reply with emojis.\"\n",
    "\n",
    "response = gemini_client.models.generate_content(\n",
    "    model='models/gemini-2.5-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=instructions\n",
    "    ),\n",
    "    contents='Hello!'\n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "007a5155-fefb-43a6-943c-ae3771c67eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"Extract the event information.\"\n",
    "content = \"Alice and Bob are going to a science fair on Friday.\"\n",
    "\n",
    "response = gemini_client.models.generate_content(\n",
    "    model='models/gemini-2.5-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=instructions,\n",
    "        response_mime_type='application/json',\n",
    "        response_json_schema=CalendarEvent.model_json_schema()\n",
    "    ),\n",
    "    contents=content\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9af0966-da20-4501-88cf-0c43d880e42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='science fair' date='Friday' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "event = CalendarEvent.model_validate_json(response.text)\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd0a564c-7bac-4075-8eae-bf483bcc55a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'd love to tell you a joke! But first, I need to know who I'm talking to. What's your name?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You're an assistant that can make jokes. Always find out the name of\n",
    "the person to make the jokes personalized. Once you know the name,\n",
    "make the joke about them.\n",
    "\"\"\".strip()\n",
    "\n",
    "content = \"tell me a joke\"\n",
    "\n",
    "response = gemini_client.models.generate_content(\n",
    "    model='models/gemini-3-flash-preview',\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_prompt,\n",
    "    ),\n",
    "    contents=content\n",
    ")\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32e39d89-0a5e-487f-8604-c0e7e1cb9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_chat = gemini_client.chats.create(\n",
    "    model='models/gemini-3-flash-preview',\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_prompt,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb652018-e40b-4243-a8f2-abb408f09714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iâ€™d love to tell you a joke! But before I do, I need to know who Iâ€™m entertaining. What is your name?\n"
     ]
    }
   ],
   "source": [
    "resp1 = gemini_chat.send_message(\"tell me a joke\")\n",
    "print(resp1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd30f8be-d312-46e2-a890-e1798b464c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Alexey! Here is a joke just for you:\n",
      "\n",
      "Why did Alexey bring a ladder to the party?\n",
      "\n",
      "Because he heard the drinks were on the house!\n"
     ]
    }
   ],
   "source": [
    "resp2 = gemini_chat.send_message(\"Alexey\")\n",
    "print(resp2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e496791e-10a4-40cb-be13-c05b8d939bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UserContent(\n",
       "   parts=[\n",
       "     Part(\n",
       "       text='tell me a joke'\n",
       "     ),\n",
       "   ],\n",
       "   role='user'\n",
       " ),\n",
       " Content(\n",
       "   parts=[\n",
       "     Part(\n",
       "       text='Iâ€™d love to tell you a joke! But before I do, I need to know who Iâ€™m entertaining. What is your name?',\n",
       "       thought_signature=b'\\x12\\xe6\\x03\\n\\xe3\\x03\\x01r\\xc8\\xda|b\\x08\"rx\\x16t%C\\\\s]\\x12\\x90/\\x9a\\xe5\\xee\\x15x\\x86kD\\xba|\\x91[\\x8d\\x87\\x9b\\x9bE\\xb1<i\\x8c\\xde\\xf3\\xb6\\xa31z\\xf5=\\x96\\xdb\\xafE\"\\xbb\\xf8C\\xd6R\\xbcD\\xce\\xd0\\\\6Q\\xd8z\\xceiN\\x17\\x94\\xe7%LL,z\\x125\\xcb\\x15\\xb5\\xcb^~\\xdcE\\x8a\\x10...'\n",
       "     ),\n",
       "   ],\n",
       "   role='model'\n",
       " ),\n",
       " UserContent(\n",
       "   parts=[\n",
       "     Part(\n",
       "       text='Alexey'\n",
       "     ),\n",
       "   ],\n",
       "   role='user'\n",
       " ),\n",
       " Content(\n",
       "   parts=[\n",
       "     Part(\n",
       "       text=\"\"\"Nice to meet you, Alexey! Here is a joke just for you:\n",
       " \n",
       " Why did Alexey bring a ladder to the party?\n",
       " \n",
       " Because he heard the drinks were on the house!\"\"\",\n",
       "       thought_signature=b'\\x12\\xaf\\x0b\\n\\xac\\x0b\\x01r\\xc8\\xda|_f\\xfe\\xcce\\xc2\\x81\\xaeh\\x888\\xc5\\xb8\\xe90h\\x1a\\xac\\xfaNtY_Y;Lx\\xae\\xe4\\x87%{\\xd9\\xbe\\xb0\\x9a\\xa2\\xe4zSU\\x81\\x8b\\x9e$S\\xf5|\\x05P\\x19\\xa6\\x8f\\xba\\xa9\\xb6X\\xe4\\xa0e\\x8c\\x02\\x85\\xa2\\xf7\\xb0P\\x8a\\x86\\x13%\\xbc6\\xa8\\x82\\xe8zcnf\\xbc.\\xd1WI\\xd8...'\n",
       "     ),\n",
       "   ],\n",
       "   role='model'\n",
       " )]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_chat.get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "476a8393-f6d0-47dc-8729-9e0c4c398d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 385 chunks from 95 documents\n"
     ]
    }
   ],
   "source": [
    "import rag\n",
    "index = rag.initialize_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48e57e31-3934-4110-a3b9-ad5409bab963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiRAG(rag.RAG):\n",
    "\n",
    "    def llm(self, user_prompt):\n",
    "        response = self.llm_client.models.generate_content(\n",
    "            model=self.model_name,\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=self.rag_instructions,\n",
    "                response_mime_type='application/json',\n",
    "                response_json_schema=self.output_type.model_json_schema()\n",
    "            ),\n",
    "            contents=user_prompt\n",
    "        )\n",
    "\n",
    "        output = self.output_type.model_validate_json(response.text)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f370acb7-c255-4f3a-aa41-57b259b58530",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_rag = GeminiRAG(\n",
    "    index,\n",
    "    gemini_client,\n",
    "    model_name='models/gemini-3-flash-preview'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b9fc628-8663-4f58-85ac-fd2ff88e8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gemini_rag.rag('llm as a judge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "676c3270-09f8-4bdb-94a3-fa1f706f4811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### LLM as a Judge Overview\n",
      "**LLM as a judge** refers to using a Large Language Model to evaluate text outputs based on custom criteria. This approach is used for regression testing, prompt comparison, or production evaluation.\n",
      "\n",
      "### Types of LLM Evaluators\n",
      "According to the documentation, there are two primary ways to use an LLM as a judge:\n",
      "- **Reference-based**: Compares new responses against a \"ground truth\" or approved reference response. This is useful for regression testing.\n",
      "- **Open-ended**: Evaluates responses based on specific criteria (e.g., verbosity or conciseness) when no reference is available.\n",
      "\n",
      "### Implementation Process\n",
      "To create an LLM judge using the `evidently` library, you follow these steps:\n",
      "1. **Define a Dataset**: Create a dataset containing inputs (questions), target responses, and the new responses to be evaluated.\n",
      "2. **Design a Prompt Template**: Use classes like `BinaryClassificationPromptTemplate` to define the evaluation criteria (e.g., correctness or conciseness). These templates handle the classification and reasoning output automatically.\n",
      "3. **Add the LLM Descriptor**: Use `LLMEval` to apply the prompt to specific columns in your dataset.\n",
      "4. **Run a Report**: Generate an Evidently `Report` using `TextEvals()` to summarize the results.\n",
      "\n",
      "### Evaluating the Judge\n",
      "Because an LLM judge is considered a small ML system, it requires its own evaluation to ensure quality:\n",
      "- **Manual Labels**: Compare the LLM's evaluations against manual \"ground truth\" labels.\n",
      "- **Classification Metrics**: Treat the judge's output as a classification problem. You can use the `ClassificationPreset()` in Evidently to calculate metrics like **accuracy**, **precision**, and **recall** (to ensure the judge doesn't miss \"incorrect\" answers).\n",
      "- **Exact Match**: Use the `ExactMatch` descriptor to directly compare the LLM-judged label against manual labels.\n"
     ]
    }
   ],
   "source": [
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6bd8032-5c5c-4ae8-8c4e-4b637445053c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How do I change the evaluator LLM model?', 'What is the difference between BinaryClassificationPromptTemplate and multi-class templates?', 'How can I create an LLM judge without writing code?', \"How do I view the reasoning behind an LLM judge's decision?\"]\n"
     ]
    }
   ],
   "source": [
    "print(response.followup_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ce646-d1cd-4a81-ab5c-f13bf7266504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
